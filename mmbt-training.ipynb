{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install madgrad\n!pip install ftfy regex tqdm\n!pip install git+https://github.com/openai/CLIP.git","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport json\nimport clip\nimport copy\nimport torch\nimport random\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport torch.nn as nn\n\nfrom PIL import Image\nfrom madgrad import MADGRAD\nfrom ast import literal_eval\nfrom tqdm.notebook import tqdm\nfrom collections import Counter\nfrom sklearn import preprocessing\nfrom matplotlib import pyplot as plt\nfrom torch.utils.data import Dataset\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\nfrom transformers import (\n    AutoConfig,\n    AutoModel,\n    AutoTokenizer,\n    MMBTConfig,\n    MMBTModel,\n    MMBTForClassification,\n    get_linear_schedule_with_warmup,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '../input/48k-imdb-movies-data/Data'\nimage_ids, names, genres, descriptions = [], [], [], []\nall_directories = os.listdir(path)\nfor directory in all_directories:\n    directories = os.listdir(os.path.join(path, directory))\n    for dir_ in directories:\n        file_path = os.path.join(path, directory, dir_, dir_ + '.json')\n        with open(file_path) as file:\n            movie = json.load(file)\n            try:\n                description = movie['description']\n                genre = movie['genre']\n                descriptions.append(description)\n                image_ids.append(dir_)\n                names.append(movie['name'])\n                genres.append(genre)\n            except KeyError:\n                continue","metadata":{"execution":{"iopub.status.busy":"2022-11-25T09:33:00.020017Z","iopub.execute_input":"2022-11-25T09:33:00.020417Z","iopub.status.idle":"2022-11-25T09:41:34.292403Z","shell.execute_reply.started":"2022-11-25T09:33:00.020384Z","shell.execute_reply":"2022-11-25T09:41:34.291381Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"path = '../input/48k-imdb-movies-with-posters/Poster'\nimage_paths = []\nall_directories = os.listdir(path)\nfor directory in all_directories:\n    directories = os.listdir(os.path.join(path, directory))\n    for dir_ in directories:\n        file_path = os.path.join(path, directory, dir_, dir_ + '.jpg')\n        image_paths.append(file_path)","metadata":{"execution":{"iopub.status.busy":"2022-11-25T09:41:34.294256Z","iopub.execute_input":"2022-11-25T09:41:34.294708Z","iopub.status.idle":"2022-11-25T09:41:46.660699Z","shell.execute_reply.started":"2022-11-25T09:41:34.294669Z","shell.execute_reply":"2022-11-25T09:41:46.659668Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"image_paths = [path for path in image_paths if path != '../input/48k-imdb-movies-with-posters/Poster/2015/tt3317562/tt3317562.jpg']\nimage_paths = pd.Series(image_paths)","metadata":{"execution":{"iopub.status.busy":"2022-11-25T09:41:46.662303Z","iopub.execute_input":"2022-11-25T09:41:46.662649Z","iopub.status.idle":"2022-11-25T09:41:46.679912Z","shell.execute_reply.started":"2022-11-25T09:41:46.662613Z","shell.execute_reply":"2022-11-25T09:41:46.678779Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"dict_ = dict(zip(image_paths.apply(lambda x: x.split('/')[5]), image_paths))","metadata":{"execution":{"iopub.status.busy":"2022-11-25T09:41:46.682447Z","iopub.execute_input":"2022-11-25T09:41:46.682830Z","iopub.status.idle":"2022-11-25T09:41:46.729212Z","shell.execute_reply.started":"2022-11-25T09:41:46.682791Z","shell.execute_reply":"2022-11-25T09:41:46.728221Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"data = pd.DataFrame({'title': names,\n                  'genres': genres,\n                  'description': descriptions,\n                  'image': image_ids})\ndata.image = data.image.map(dict_)\ndata.dropna(inplace=True)\ndata = data.sample(frac=1)\ndata.reset_index(drop=True, inplace=True)\ndata = data.explode('genres')\ndata = data.groupby(['title', 'description', 'image']).agg({'genres': lambda x: x.tolist()}).reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-11-25T09:43:24.880517Z","iopub.execute_input":"2022-11-25T09:43:24.880876Z","iopub.status.idle":"2022-11-25T09:43:25.539418Z","shell.execute_reply.started":"2022-11-25T09:43:24.880842Z","shell.execute_reply":"2022-11-25T09:43:25.538436Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"mlb = MultiLabelBinarizer(sparse_output=True)\ndata = data.join(pd.DataFrame.sparse.from_spmatrix(\n                mlb.fit_transform(data['genres']),\n                index=data.index,\n                columns=mlb.classes_)[targets])","metadata":{"execution":{"iopub.status.busy":"2022-11-25T09:43:26.778937Z","iopub.execute_input":"2022-11-25T09:43:26.779646Z","iopub.status.idle":"2022-11-25T09:43:26.909202Z","shell.execute_reply.started":"2022-11-25T09:43:26.779607Z","shell.execute_reply":"2022-11-25T09:43:26.908154Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"targets = ['Action', 'Crime', 'Adventure', 'Thriller', 'Drama', 'Family',\n           'Sport', 'Mystery', 'Western', 'History', 'Sci-Fi', 'Animation',\n           'Documentary', 'Music', 'War', 'Biography', 'Musical', 'Superhero',\n           'Horror', 'Short', 'Comedy', 'Fantasy', 'Romance', 'Film-Noir']","metadata":{"execution":{"iopub.status.busy":"2022-11-25T09:43:13.048706Z","iopub.execute_input":"2022-11-25T09:43:13.049058Z","iopub.status.idle":"2022-11-25T09:43:13.054588Z","shell.execute_reply.started":"2022-11-25T09:43:13.049027Z","shell.execute_reply":"2022-11-25T09:43:13.053603Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"data['concat'] = data['title'] + ' | ' + data['description'] \ndata.reset_index(inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-11-25T09:44:48.891987Z","iopub.execute_input":"2022-11-25T09:44:48.892858Z","iopub.status.idle":"2022-11-25T09:44:48.899852Z","shell.execute_reply.started":"2022-11-25T09:44:48.892816Z","shell.execute_reply":"2022-11-25T09:44:48.898751Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"clip_model, preprocess = clip.load(\"RN50x4\", device=device, jit=False)\n\nfor p in clip_model.parameters():\n    p.requires_grad = False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_image_embeds = 4\nnum_labels = 24\ngradient_accumulation_steps = 20\nmax_seq_length = 80 \nmax_grad_norm = 0.5\ntrain_batch_size = 16\neval_batch_size = 16\nimage_encoder_size = 288\nimage_features_size = 640\nnum_train_epochs = 5","metadata":{"execution":{"iopub.status.busy":"2022-11-25T09:44:48.901212Z","iopub.execute_input":"2022-11-25T09:44:48.901643Z","iopub.status.idle":"2022-11-25T09:45:06.652483Z","shell.execute_reply.started":"2022-11-25T09:44:48.901596Z","shell.execute_reply":"2022-11-25T09:45:06.651477Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"100%|███████████████████████████████████████| 402M/402M [00:06<00:00, 65.9MiB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"def slice_image(im, desired_size):\n    old_size = im.size\n    ratio = float(desired_size) / min(old_size)\n    new_size = tuple([int(x * ratio) for x in old_size])\n    im = im.resize(new_size, Image.ANTIALIAS)    \n    ar = np.array(im)\n    images = []\n    if ar.shape[0] < ar.shape[1]:\n        middle = ar.shape[1] // 2\n        half = desired_size // 2\n        images.append(Image.fromarray(ar[:, :desired_size]))\n        images.append(Image.fromarray(ar[:, middle-half:middle+half]))\n        images.append(Image.fromarray(ar[:, ar.shape[1]-desired_size:ar.shape[1]]))\n    else:\n        middle = ar.shape[0] // 2\n        half = desired_size // 2\n        images.append(Image.fromarray(ar[:desired_size, :]))\n        images.append(Image.fromarray(ar[middle-half:middle+half, :]))\n        images.append(Image.fromarray(ar[ar.shape[0]-desired_size:ar.shape[0], :]))\n    return images\n  \ndef resize_pad_image(im, desired_size):\n    old_size = im.size\n    ratio = float(desired_size) / max(old_size)\n    new_size = tuple([int(x * ratio) for x in old_size])\n    im = im.resize(new_size, Image.ANTIALIAS)\n    new_im = Image.new(\"RGB\", (desired_size, desired_size))\n    new_im.paste(im, ((desired_size - new_size[0]) // 2,\n                        (desired_size - new_size[1]) // 2))\n    return new_im","metadata":{"execution":{"iopub.status.busy":"2022-11-25T09:46:06.352253Z","iopub.execute_input":"2022-11-25T09:46:06.352566Z","iopub.status.idle":"2022-11-25T09:46:06.365104Z","shell.execute_reply.started":"2022-11-25T09:46:06.352538Z","shell.execute_reply":"2022-11-25T09:46:06.364101Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class ClipEncoderMulti(nn.Module):\n    def __init__(self, num_embeds, num_features=image_features_size):\n        super().__init__()        \n        self.model = clip_model\n        self.num_embeds = num_embeds\n        self.num_features = num_features\n\n    def forward(self, x):\n        # 4x3x288x288 -> 1x4x640\n        out = self.model.encode_image(x.view(-1, 3, 288, 288))\n        out = out.view(-1, self.num_embeds, self.num_features).float()\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-11-25T09:46:07.639498Z","iopub.execute_input":"2022-11-25T09:46:07.640615Z","iopub.status.idle":"2022-11-25T09:46:07.648174Z","shell.execute_reply.started":"2022-11-25T09:46:07.640573Z","shell.execute_reply":"2022-11-25T09:46:07.647153Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class JsonlDataset(Dataset):\n    def __init__(self, data_path, tokenizer, transforms, max_seq_length):\n        self.data = [json.loads(l) for l in open(data_path)]\n        self.data_dir = os.path.dirname(data_path)\n        self.tokenizer = tokenizer\n        self.max_seq_length = max_seq_length\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        sentence = torch.LongTensor(self.tokenizer.encode(self.data[index][\"text\"], add_special_tokens=True))\n        start_token, sentence, end_token = sentence[0], sentence[1:-1], sentence[-1]\n        sentence = sentence[:self.max_seq_length]\n\n        label = torch.FloatTensor([self.data[index][\"label\"]])\n\n        image = Image.open(os.path.join(self.data_dir, self.data[index][\"img\"])).convert(\"RGB\")\n        sliced_images = slice_image(image, 288)\n        sliced_images = [np.array(self.transforms(im)) for im in sliced_images]\n        image = resize_pad_image(image, image_encoder_size)\n        image = np.array(self.transforms(image))\n        \n        sliced_images = [image] + sliced_images         \n        sliced_images = torch.from_numpy(np.array(sliced_images)).to(device)\n\n        return {\"image_start_token\": start_token,            \n                \"image_end_token\": end_token,\n                \"sentence\": sentence,\n                \"image\": sliced_images,\n                \"label\": label}\n\n    def get_label_frequencies(self):\n        label_freqs = Counter()\n        for row in self.data:\n            label_freqs.update([row[\"label\"]])\n        return label_freqs\n    \n    def get_labels(self):\n        labels = []\n        for row in self.data:\n            labels.append(row[\"label\"])\n        return labels\n   \n\ndef collate_fn(batch):\n    lens = [len(row[\"sentence\"]) for row in batch]\n    bsz, max_seq_len = len(batch), max(lens)\n\n    mask_tensor = torch.zeros(bsz, max_seq_len, dtype=torch.long)\n    text_tensor = torch.zeros(bsz, max_seq_len, dtype=torch.long)\n\n    for i_batch, (input_row, length) in enumerate(zip(batch, lens)):\n        text_tensor[i_batch, :length] = input_row[\"sentence\"]\n        mask_tensor[i_batch, :length] = 1\n    \n    img_tensor = torch.stack([row[\"image\"] for row in batch])\n    tgt_tensor = torch.stack([row[\"label\"] for row in batch])\n    img_start_token = torch.stack([row[\"image_start_token\"] for row in batch])\n    img_end_token = torch.stack([row[\"image_end_token\"] for row in batch])\n\n    return text_tensor, mask_tensor, img_tensor, img_start_token, img_end_token, tgt_tensor","metadata":{"execution":{"iopub.status.busy":"2022-11-25T09:46:11.397232Z","iopub.execute_input":"2022-11-25T09:46:11.397811Z","iopub.status.idle":"2022-11-25T09:46:11.412569Z","shell.execute_reply.started":"2022-11-25T09:46:11.397774Z","shell.execute_reply":"2022-11-25T09:46:11.411414Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def load_examples(tokenizer, evaluate=False):\n    path = \"dev_seen_clean.jsonl\" if evaluate else f\"train_augmented.jsonl\"\n    transforms = preprocess\n    dataset = JsonlDataset(path, tokenizer, transforms, max_seq_length - num_image_embeds - 2)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-11-25T09:46:11.657762Z","iopub.execute_input":"2022-11-25T09:46:11.658180Z","iopub.status.idle":"2022-11-25T09:46:11.666548Z","shell.execute_reply.started":"2022-11-25T09:46:11.658116Z","shell.execute_reply":"2022-11-25T09:46:11.665136Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"model_name = 'bert-base-multilingual-cased'\ntransformer_config = AutoConfig.from_pretrained(model_name) \ntransformer = AutoModel.from_pretrained(model_name, config=transformer_config)\nimg_encoder = ClipEncoderMulti(num_image_embeds)\ntokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n\nconfig = MMBTConfig(transformer_config, num_labels=num_classes, modal_hidden_size=image_features_size)\nmodel = MMBTForClassification(config, transformer, img_encoder)\nmodel.to(device) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(data[['index', 'genres', 'concat', 'image']], data[targets])","metadata":{"execution":{"iopub.status.busy":"2022-11-25T09:50:08.133805Z","iopub.execute_input":"2022-11-25T09:50:08.134209Z","iopub.status.idle":"2022-11-25T09:50:08.281287Z","shell.execute_reply.started":"2022-11-25T09:50:08.134173Z","shell.execute_reply":"2022-11-25T09:50:08.280252Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"test = []\nfor row_x, row_y in zip(x_test.iterrows(), y_test.iterrows()):\n    id_ = row_x[1]['index']\n    genre = row_x[1]['genres']\n    image_path = row_x[1]['image']\n    test.append({'id':id_,\n                 'img': image_path,\n                 'label':row_y[1].to_list(),\n                 'text':row_x[1]['concat']})\n    \ntrain = []\nfor row_x, row_y in zip(x_train.iterrows(), y_train.iterrows()):\n    id_ = row_x[1]['index']\n    genre = row_x[1]['genres']\n    image_path = row_x[1]['image']\n    train.append({'id':id_,\n                 'img': image_path,\n                 'label':row_y[1].to_list(),\n                 'text':row_x[1]['concat']})","metadata":{"execution":{"iopub.status.busy":"2022-11-25T09:52:05.401961Z","iopub.execute_input":"2022-11-25T09:52:05.402679Z","iopub.status.idle":"2022-11-25T09:52:10.250868Z","shell.execute_reply.started":"2022-11-25T09:52:05.402638Z","shell.execute_reply":"2022-11-25T09:52:10.249869Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"with open('./train_augmented.jsonl', 'w') as outfile:\n    for entry in train:\n        json.dump(entry, outfile)\n        outfile.write('\\n')\n        \nwith open('./dev_seen_clean.jsonl', 'w') as outfile:\n    for entry in test:\n        json.dump(entry, outfile)\n        outfile.write('\\n')","metadata":{"execution":{"iopub.status.busy":"2022-11-25T09:52:38.996654Z","iopub.execute_input":"2022-11-25T09:52:38.997030Z","iopub.status.idle":"2022-11-25T09:52:40.573469Z","shell.execute_reply.started":"2022-11-25T09:52:38.996995Z","shell.execute_reply":"2022-11-25T09:52:40.572509Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"train_dataset = load_examples(tokenizer, evaluate=False)\neval_dataset = load_examples(tokenizer, evaluate=True)   \n\ntrain_sampler = RandomSampler(train_dataset)\neval_sampler = SequentialSampler(eval_dataset)\n\ntrain_dataloader = DataLoader(train_dataset,\n                              sampler=train_sampler,\n                              batch_size=train_batch_size,\n                              collate_fn=collate_fn)\n\neval_dataloader = DataLoader(eval_dataset, \n                             sampler=eval_sampler, \n                             batch_size=eval_batch_size, \n                             collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2022-11-25T09:52:45.100510Z","iopub.execute_input":"2022-11-25T09:52:45.100871Z","iopub.status.idle":"2022-11-25T09:52:45.431046Z","shell.execute_reply.started":"2022-11-25T09:52:45.100838Z","shell.execute_reply":"2022-11-25T09:52:45.430074Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"no_decay = [\"bias\", \"LayerNorm.weight\"]\nweight_decay = 0.0005\noptimizer_grouped_parameters = [{\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n                                 \"weight_decay\": weight_decay},\n                                {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n                                 \"weight_decay\": 0.0}]\n\nt_total = (len(train_dataloader) // gradient_accumulation_steps) * num_train_epochs\nwarmup_steps = t_total // 10\noptimizer = MADGRAD(optimizer_grouped_parameters, lr=2e-4)\nscheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, t_total)\ncriterion = nn.BCEWithLogitsLoss()","metadata":{"execution":{"iopub.status.busy":"2022-11-25T09:57:34.173435Z","iopub.execute_input":"2022-11-25T09:57:34.173804Z","iopub.status.idle":"2022-11-25T09:57:34.191406Z","shell.execute_reply.started":"2022-11-25T09:57:34.173771Z","shell.execute_reply":"2022-11-25T09:57:34.190223Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, tokenizer, criterion, dataloader, tres = 0.5): \n    print('eval')\n    eval_loss = 0.0\n    nb_eval_steps = 0\n    preds = None\n    proba = None\n    out_label_ids = None\n    for batch in dataloader:\n        model.eval()\n        batch = tuple(t.to(device) for t in batch)\n        with torch.no_grad():\n            labels = batch[5]\n            inputs = {\"input_ids\": batch[0],\n                      \"input_modal\": batch[2],\n                      \"attention_mask\": batch[1],\n                      \"modal_start_tokens\": batch[3],\n                      \"modal_end_tokens\": batch[4],\n                      \"return_dict\": False}\n            outputs = model(**inputs)\n            logits = outputs[0]\n            tmp_eval_loss = criterion(logits, labels.reshape(-1, 23))\n            eval_loss += tmp_eval_loss.mean().item()\n        nb_eval_steps += 1\n        if preds is None:\n            preds = torch.sigmoid(logits).detach().cpu().numpy() > tres\n            proba = torch.sigmoid(logits).detach().cpu().numpy()\n            out_label_ids = labels.detach().cpu().numpy()\n        else:            \n            preds = np.append(preds, torch.sigmoid(logits).detach().cpu().numpy() > tres, axis=0)\n            proba = np.append(proba, torch.sigmoid(logits).detach().cpu().numpy(), axis=0)\n            out_label_ids = np.append(out_label_ids, labels.detach().cpu().numpy(), axis=0)\n    \n    eval_loss = eval_loss / nb_eval_steps\n    result = {\"loss\": eval_loss,\n              \"micro_f1\": f1_score(out_label_ids.reshape(-1, 23), preds, average='micro'),\n              \"prediction\": preds,\n              \"labels\": out_label_ids,\n              \"proba\": proba}\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-11-25T09:57:41.929348Z","iopub.execute_input":"2022-11-25T09:57:41.929703Z","iopub.status.idle":"2022-11-25T09:57:41.942884Z","shell.execute_reply.started":"2022-11-25T09:57:41.929669Z","shell.execute_reply":"2022-11-25T09:57:41.941944Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"optimizer_step = 0\nglobal_step = 0\ntrain_step = 0\ntr_loss, logging_loss = 0.0, 0.0\nglobal_steps_list = []\ntrain_loss_list = []\nval_loss_list = []\nval_f1_list = []\neval_every = 2217\nfile_path = \"\"","metadata":{"execution":{"iopub.status.busy":"2022-11-25T09:57:44.503503Z","iopub.execute_input":"2022-11-25T09:57:44.503865Z","iopub.status.idle":"2022-11-25T09:57:44.510257Z","shell.execute_reply.started":"2022-11-25T09:57:44.503834Z","shell.execute_reply":"2022-11-25T09:57:44.508983Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"model.zero_grad()\n\nfor i in range(num_train_epochs):\n    print(\"Epoch\", i + 1, f\"from {num_train_epochs}\")\n    whole_y_pred = np.array([])\n    whole_y_t = np.array([])\n    for step, batch in enumerate(tqdm(train_dataloader)):\n        model.train()\n        batch = tuple(t.to(device) for t in batch)\n        labels = batch[5]\n        inputs = {\"input_ids\": batch[0],\n                  \"input_modal\": batch[2],\n                  \"attention_mask\": batch[1],\n                  \"modal_start_tokens\": batch[3],\n                  \"modal_end_tokens\": batch[4],\n                  \"return_dict\": False}\n        outputs = model(**inputs)\n        logits = outputs[0]\n        loss = criterion(logits, labels.reshape(-1, 24))        \n        if gradient_accumulation_steps > 1:\n            loss = loss / gradient_accumulation_steps\n        loss.backward()\n        \n        tr_loss += loss.item()\n        running_loss += loss.item()\n        global_step += 1\n        \n        if (step + 1) % gradient_accumulation_steps == 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n            optimizer.step()\n            scheduler.step()    \n            \n            optimizer_step += 1\n            optimizer.zero_grad()   \n                        \n        if (step + 1) % eval_every == 0:\n            \n            average_train_loss = running_loss / eval_every\n            train_loss_list.append(average_train_loss)\n            global_steps_list.append(global_step)\n            running_loss = 0.0  \n            \n            val_result = evaluate(model, tokenizer, criterion, eval_dataloader)\n            \n            val_loss_list.append(val_result['loss'])\n            val_f1_list.append(val_result['micro_f1'])\n\n            print(\"Train loss:\", f\"{average_train_loss:.4f}\", \n                  \"Val loss:\", f\"{val_result['loss']:.4f}\",\n                  \"Val f1:\", f\"{val_result['micro_f1']:.4f}\")   \n    print('\\n')     ","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}