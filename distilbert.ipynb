{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport os\nimport nltk\nimport time\nimport json\nimport torch\nimport random\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom tqdm import tqdm\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom transformers import DistilBertTokenizer, DistilBertModel\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler","metadata":{"execution":{"iopub.status.busy":"2022-11-24T21:05:08.287575Z","iopub.execute_input":"2022-11-24T21:05:08.287936Z","iopub.status.idle":"2022-11-24T21:05:08.304358Z","shell.execute_reply.started":"2022-11-24T21:05:08.287881Z","shell.execute_reply":"2022-11-24T21:05:08.303426Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words = stopwords.words(\"english\")\nlemma = nltk.stem.WordNetLemmatizer()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '../input/48k-imdb-movies-data/Data'\nimages, names, genres, descriptions = [], [], [], []\nall_directories = os.listdir(path)\nfor directory in all_directories:\n    directories = os.listdir(os.path.join(path, directory))\n    for dir_ in directories:\n        file_path = os.path.join(path, directory, dir_, dir_ + '.json')\n        with open(file_path) as file:\n            movie = json.load(file)\n            try:\n                description = movie['description']\n                genre = movie['genre']\n                descriptions.append(description)\n                images.append(dir_)\n                names.append(movie['name'])\n                genres.append(genre)\n            except KeyError:\n                continue","metadata":{"execution":{"iopub.status.busy":"2022-11-24T21:05:17.108859Z","iopub.execute_input":"2022-11-24T21:05:17.109245Z","iopub.status.idle":"2022-11-24T21:10:18.180452Z","shell.execute_reply.started":"2022-11-24T21:05:17.109205Z","shell.execute_reply":"2022-11-24T21:10:18.179549Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data = pd.DataFrame({'title': names,\n                  'genres': genres,\n                  'description': descriptions,\n                  'image': images})\ndata = data.sample(frac=1)\ndata.reset_index(drop=True, inplace=True)\ndata = data.explode('genres')\ndata = data.groupby(['title', 'description']).agg({'genres': lambda x: x.tolist()}).reset_index()\ndata['concat'] = data['title'] + ' [SEP] ' + data['description']","metadata":{"execution":{"iopub.status.busy":"2022-11-24T21:10:29.784486Z","iopub.execute_input":"2022-11-24T21:10:29.784806Z","iopub.status.idle":"2022-11-24T21:10:33.888143Z","shell.execute_reply.started":"2022-11-24T21:10:29.784774Z","shell.execute_reply":"2022-11-24T21:10:33.887268Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"mlb = MultiLabelBinarizer(sparse_output=True)\ndata = data.join(pd.DataFrame.sparse.from_spmatrix(\n                mlb.fit_transform(data['genres']),\n                index=data.index,\n                columns=mlb.classes_))","metadata":{"execution":{"iopub.status.busy":"2022-11-24T21:10:33.891526Z","iopub.execute_input":"2022-11-24T21:10:33.891804Z","iopub.status.idle":"2022-11-24T21:10:33.997626Z","shell.execute_reply.started":"2022-11-24T21:10:33.891778Z","shell.execute_reply":"2022-11-24T21:10:33.996825Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"targets = ['Action', 'Crime', 'Adventure', 'Thriller', 'Drama', 'Family',\n           'Sport', 'Mystery', 'Western', 'History', 'Sci-Fi', 'Animation',\n           'Documentary', 'Music', 'War', 'Biography', 'Musical', 'Superhero',\n           'Horror', 'Short', 'Comedy', 'Fantasy', 'Romance', 'Film-Noir']","metadata":{"execution":{"iopub.status.busy":"2022-11-24T21:10:50.426264Z","iopub.execute_input":"2022-11-24T21:10:50.426676Z","iopub.status.idle":"2022-11-24T21:10:50.432006Z","shell.execute_reply.started":"2022-11-24T21:10:50.426631Z","shell.execute_reply":"2022-11-24T21:10:50.430674Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"X = data.concat.values\ny = data[targets].values\n\nX_train, X_val, y_train, y_val =train_test_split(X, y, test_size=0.1, random_state=2020)","metadata":{"execution":{"iopub.status.busy":"2022-11-24T21:10:51.707267Z","iopub.execute_input":"2022-11-24T21:10:51.707576Z","iopub.status.idle":"2022-11-24T21:10:51.741835Z","shell.execute_reply.started":"2022-11-24T21:10:51.707546Z","shell.execute_reply":"2022-11-24T21:10:51.741083Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_preprocessing(text):\n    text = text.lower()\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"won't\", \"will not \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"can not \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n    text = re.sub(r\"\\'\\n\", \" \", text)\n    text = re.sub(r\"-\", \" \", text)\n    text = re.sub(r\"\\'\\xa0\", \" \", text)\n    text = re.sub('\\s+', ' ', text)\n    text = ''.join(c for c in text if not c.isnumeric())\n    text = re.sub(r'&amp;', '&', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-11-24T21:10:55.414258Z","iopub.execute_input":"2022-11-24T21:10:55.414514Z","iopub.status.idle":"2022-11-24T21:10:55.425878Z","shell.execute_reply.started":"2022-11-24T21:10:55.414489Z","shell.execute_reply":"2022-11-24T21:10:55.424924Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"tokenizer = DistilBertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessing_for_bert(data):\n    input_ids = []\n    attention_masks = []\n\n    for sent in data:\n        encoded_sent = tokenizer.encode_plus(text=text_preprocessing(sent),\n                                             add_special_tokens=True,\n                                             max_length=MAX_LEN,\n                                             pad_to_max_length=True,\n                                             return_attention_mask=True)\n\n        input_ids.append(encoded_sent.get('input_ids'))\n        attention_masks.append(encoded_sent.get('attention_mask'))\n\n    input_ids = torch.tensor(input_ids)\n    attention_masks = torch.tensor(attention_masks)\n    return input_ids,attention_masks","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 256\n\ntrain_inputs, train_masks = preprocessing_for_bert(X_train)\nval_inputs, val_masks = preprocessing_for_bert(X_val)","metadata":{"execution":{"iopub.status.busy":"2022-11-24T21:11:02.895558Z","iopub.execute_input":"2022-11-24T21:11:02.895880Z","iopub.status.idle":"2022-11-24T21:11:02.900856Z","shell.execute_reply.started":"2022-11-24T21:11:02.895849Z","shell.execute_reply":"2022-11-24T21:11:02.899901Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"batch_size = 16\n\ntrain_labels = torch.tensor(y_train)\nval_labels = torch.tensor(y_val)\n\ntrain_data = TensorDataset(train_inputs,train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nval_data = TensorDataset(val_inputs, val_masks, val_labels)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-11-24T21:12:12.792848Z","iopub.execute_input":"2022-11-24T21:12:12.793216Z","iopub.status.idle":"2022-11-24T21:12:12.811186Z","shell.execute_reply.started":"2022-11-24T21:12:12.793182Z","shell.execute_reply":"2022-11-24T21:12:12.810487Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class BertClassifier(nn.Module):\n    def __init__(self, freeze_bert=False):\n        super(BertClassifier,self).__init__()\n        D_in, H, D_out = 768, 30, 24\n        self.bert = DistilBertModel.from_pretrained(\"bert-base-uncased\")\n        self.classifier = nn.Sequential(\n                            nn.Linear(D_in, H),\n                            nn.ReLU(),\n                            nn.Linear(H, D_out))\n        self.sigmoid = nn.Sigmoid()\n        if freeze_bert:\n            for param in self.bert.parameters():\n                param.requires_grad=False\n    \n    def forward(self,input_ids,attention_mask):\n        outputs = self.bert(input_ids=input_ids,\n                           attention_mask = attention_mask)\n\n        last_hidden_state_cls = outputs[0][:,0,:]\n        logit = self.classifier(last_hidden_state_cls)\n        return logit","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def initialize_model(epochs=4):\n    bert_classifier = BertClassifier(freeze_bert=False)\n    bert_classifier.to(device)\n    optimizer = AdamW(bert_classifier.parameters(),\n                      lr=5e-5,\n                      eps=1e-8)\n    total_steps = len(train_dataloader) * epochs\n    scheduler = get_linear_schedule_with_warmup(optimizer, \n                                                num_warmup_steps=0,\n                                                num_training_steps=total_steps)\n    return bert_classifier, optimizer, scheduler","metadata":{"execution":{"iopub.status.busy":"2022-11-24T21:12:16.461773Z","iopub.execute_input":"2022-11-24T21:12:16.462144Z","iopub.status.idle":"2022-11-24T21:12:16.468030Z","shell.execute_reply.started":"2022-11-24T21:12:16.462112Z","shell.execute_reply":"2022-11-24T21:12:16.467183Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed_value=42):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)\n\ndef train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n    print(\"Start training...\\n\")\n    for epoch_i in range(epochs):\n        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'F1-score':^9} | {'Elapsed':^9}\")\n        print(\"-\" * 70)\n        t0_epoch, t0_batch = time.time(), time.time()\n        total_loss, batch_loss, batch_counts = 0, 0, 0\n        model.train()\n        for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n            batch_counts += 1\n            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n            model.zero_grad()\n            logits = model(b_input_ids, b_attn_mask)\n            loss = loss_fn(logits, b_labels.float())\n            batch_loss += loss.item()\n            total_loss += loss.item()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            if (step % 50000 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n                time_elapsed = time.time() - t0_batch\n                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n                batch_loss, batch_counts = 0, 0\n                t0_batch = time.time()\n        avg_train_loss = total_loss / len(train_dataloader)\n\n        print(\"-\" * 70)\n        if evaluation == True:\n            val_loss, val_f1_score = evaluate(model, val_dataloader)\n            time_elapsed = time.time() - t0_epoch\n            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_f1_score:^9.2f} | {time_elapsed:^9.2f}\")\n            print(\"-\" * 70)\n        print(\"\\n\")\n    print(\"Training complete!\")\n\n\ndef evaluate(model, val_dataloader):\n    model.eval()\n    val_f1_score = []\n    val_loss = []\n    for batch in val_dataloader:\n        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n        loss = loss_fn(logits, b_labels.float())\n        val_loss.append(loss.item())\n        threshold = 0.5\n        f1_score_ = f1_score(np.rint(logits.cpu().view(-1, 23).numpy()) > threshold,\n                            b_labels.cpu().view(-1, 23).numpy(),\n                            average='micro')\n        val_f1_score.append(f1_score_)\n    val_loss = np.mean(val_loss)\n    val_f1_score = np.mean(val_f1_score)\n    return val_loss, val_f1_score","metadata":{"execution":{"iopub.status.busy":"2022-11-24T21:12:21.555002Z","iopub.execute_input":"2022-11-24T21:12:21.555332Z","iopub.status.idle":"2022-11-24T21:12:21.577550Z","shell.execute_reply.started":"2022-11-24T21:12:21.555301Z","shell.execute_reply":"2022-11-24T21:12:21.576092Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache() ","metadata":{"execution":{"iopub.status.busy":"2022-11-24T21:12:26.946990Z","iopub.execute_input":"2022-11-24T21:12:26.947310Z","iopub.status.idle":"2022-11-24T21:12:26.952554Z","shell.execute_reply.started":"2022-11-24T21:12:26.947278Z","shell.execute_reply":"2022-11-24T21:12:26.951388Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"set_seed(42)\nloss_fn = nn.BCEWithLogitsLoss()\nbert_classifier, optimizer, scheduler = initialize_model(epochs=4)  \ntrain(bert_classifier, train_dataloader, val_dataloader, epochs=4, evaluation=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}